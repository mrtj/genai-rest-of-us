{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question answering with Retrieval Augmented Generation (RAG)\n",
    "\n",
    "This notebook provides a practical example of how to build a question answering system using Retrieval Augmented Generation. To create a RAG-based Q&A tool, you need to do two main things: first, index your own set of documents for the system to use, and second, set up a way to retrieve the right documents when the application needs to answer a question.\n",
    "\n",
    "## Overview of document indexing\n",
    "\n",
    "Indexing your documents involves several steps:\n",
    "\n",
    "1. Gather the documents you want to use.\n",
    "2. Change these documents into simple text or markdown format.\n",
    "3. Break the text down into smaller parts called \"chunks\".\n",
    "4. Create document embeddings for these chunks.\n",
    "5. Load the embeddings into a vector store system.\n",
    "\n",
    "<img src=\"./img/rag_indexing.png \" alt=\"RAG indexing overview\" width=\"800\"/>\n",
    "\n",
    "## Overview of document retrieval\n",
    "\n",
    "At retrieval time, the system will calculate the embeddings of the query phrase. The vector store will look up for document chunks semantically similar to the query, based on k-nearest-neighbor search algorithm over the document embeddings. The most relevant document chunks will be passed to a large language model (LLM) together with the instruction to answer the original question based on the retrieved chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/svg/JSV7aW5pdDogeyd0aGVtZSc6J2xpZ2h0J30gfSUlCgogICAgZ3JhcGggTFIKICAgICAgICBxdWVzdGlvbihRdWVzdGlvbikgLS0+IHByb21wdChQcm9tcHQpCiAgICAgICAgcXVlc3Rpb24gLS0+IHZlY3Rvcl9zdG9yZShWZWN0b3IgU3RvcmUpCiAgICAgICAgdmVjdG9yX3N0b3JlIC0tIFJldHJpZXZlZDxici8+ZG9jdW1lbnRzIC0tPiBwcm9tcHQoUHJvbXB0KQogICAgICAgIHByb21wdChQcm9tcHQpIC0tPiBMTE0KICAgICAgICBMTE0gLS0+IHJlc3BvbnNlKFJlc3BvbnNlKQogICAg?bgColor=!white\" width=\"800\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diagrams import rag\n",
    "rag(theme=\"light\", background_color=\"!white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document preprocessing\n",
    "\n",
    "### Acquire source document from the web and convert it into plain text\n",
    "\n",
    "The following code snippet does three things: it gets a webpage from the internet, reads its HTML code, and then pulls out the main text from that page.\n",
    "\n",
    "It's worth mentioning that Wikipedia has a public API that lets you easily download page contents in a structured format. But in this example, for the sake of this demonstration, we'll read the webpage's HTML code directly, just like how it looks in a web browser. On Wikipedia pages, the main part of the article is usually in a section marked with `div` tag and an attribute `id=\"bodyContent\"`. Our code is designed to find this part and take out the text, leaving behind any other HTML code inside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document loader has returned 1 documents.\n",
      "The beginning of the first document:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " \n",
      " \n",
      " Coordinates :  45°11′12″N   9°9′23″E ﻿ / ﻿ 45.18667°N 9.15639°E ﻿ /  45.18667; 9.15639 \n",
      " \n",
      " From Wikipedia, the free encyclopedia \n",
      " \n",
      " \n",
      " Public university in Pavia, Italy \n",
      " University of Pavia Università di Pavia Seal  of the University of Pavia Latin :  Alma Ticinensis Universitas Type Public Established 13 April 1361 ; 662 years ago  ( 1361-04-13 ) Rector Francesco Svelto Academic staff 981 Students 23,849 Undergraduates 11,983 Postgraduates 9,366 Location Pavia ,  Italy 45°11′12″N   9°9′23″E ﻿ / ﻿ 45.18667°N 9.15639°E ﻿ /  45.18667; 9.15639 Campus Urban/University town Colors    Pavia Yellow Affiliations Coimbra Group ,  EUA ,  Netval Website unipv .eu \n",
      " The  University of Pavia  ( Italian :  Università degli Studi di Pavia ,  UNIPV  or  Università di Pavia ;  Latin :  Alma  Ticinensis  Universitas ) is a university located in  Pavia ,  Lombardy ,  Italy . There was evidence of teaching as early as 1361, making it one of the  oldest universities  in the world. It was the sole university in  Milan  and the greater  Lombardy  region until the end of the 19th century. [1] \n",
      "In 2022 the university was recognized by the  Times Higher Education  among the top 10 in Italy and among the 300 best in the world. [2] \n",
      "Currently, it has 18 departments and 9 faculties. It does not have a main campus; its buildings and facilities are scattered around the city, which is in turn called \"a city campus.\" The university caters to more than 20,000 students who come from Italy and all over t\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "doc_url = \"https://en.wikipedia.org/wiki/University_of_Pavia\"\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(doc_url,),\n",
    "    bs_kwargs=dict(parse_only=bs4.SoupStrainer(id=(\"bodyContent\"))),\n",
    "    bs_get_text_kwargs=dict(separator=\" \")\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"Document loader has returned {len(docs)} documents.\")\n",
    "print(\"The beginning of the first document:\")\n",
    "print(\"-\" * 80)\n",
    "print(docs[0].page_content[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the document into chunks\n",
    "\n",
    "The following code snippet splits the source document into chunks. It keeps dividing the document based on meaningful breaks (like double new lines, single new lines, punctuation, and spaces between words) until each piece is smaller than the maximum chunk size. It also keeps a 20% of overlap between chunks.\n",
    "\n",
    "It is worth noting that the length of a document can be measured in various ways. The most common method is counting the number of characters, and this is what langchain text splitters usually do by default. However, many large language models (LLMs) process the input text differently. They use a tool called a _tokenizer_ to break down the text and then measure its length in _number of tokens_ instead of characters. For instance, OpenAI models use a tokenizer known as `tiktoken`. To accurately determine the length of a prompt, it's essential to measure it in terms of the number of tokens. Fortunately, langchain is equipped with a feature that allows for measuring chunk lengths in terms of `tiktoken` token length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document was split into 50 chunks.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 500\n",
    "chunk_overlap_rate = 0.2\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=chunk_size, chunk_overlap=int(chunk_size * chunk_overlap_rate)\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "print(f\"The document was split into {len(splits)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first two chunks:\n",
      "--------------------------------------------------------------------------------\n",
      "Coordinates :  45°11′12″N   9°9′23″E ﻿ / ﻿ 45.18667°N 9.15639°E ﻿ /  45.18667; 9.15639 \n",
      " \n",
      " From Wikipedia, the free encyclopedia \n",
      " \n",
      " \n",
      " Public university in Pavia, Italy \n",
      " University of Pavia Università di Pavia Seal  of the University of Pavia Latin :  Alma Ticinensis Universitas Type Public Established 13 April 1361 ; 662 years ago  ( 1361-04-13 ) Rector Francesco Svelto Academic staff 981 Students 23,849 Undergraduates 11,983 Postgraduates 9,366 Location Pavia ,  Italy 45°11′12″N   9°9′23″E ﻿ / ﻿ 45.18667°N 9.15639°E ﻿ /  45.18667; 9.15639 Campus Urban/University town Colors    Pavia Yellow Affiliations Coimbra Group ,  EUA ,  Netval Website unipv .eu \n",
      " The  University of Pavia  ( Italian :  Università degli Studi di Pavia ,  UNIPV  or  Università di Pavia ;  Latin :  Alma  Ticinensis  Universitas ) is a university located in  Pavia ,  Lombardy ,  Italy . There was evidence of teaching as early as 1361, making it one of the  oldest universities  in the world. It was the sole university in  Milan  and the greater  Lombardy  region until the end of the 19th century. [1] \n",
      "In 2022 the university was recognized by the  Times Higher Education  among the top 10 in Italy and among the 300 best in the world. [2] \n",
      "Currently, it has 18 departments and 9 faculties. It does not have a main campus; its buildings and facilities are scattered around the city, which is in turn called \"a city campus.\" The university caters to more than 20,000 students who come from Italy and all over the world.\n",
      "--------------------------------------------------------------------------------\n",
      "In 2022 the university was recognized by the  Times Higher Education  among the top 10 in Italy and among the 300 best in the world. [2] \n",
      "Currently, it has 18 departments and 9 faculties. It does not have a main campus; its buildings and facilities are scattered around the city, which is in turn called \"a city campus.\" The university caters to more than 20,000 students who come from Italy and all over the world.\n",
      " The university offers more than 80 undergraduate programs; over 40 master programs, and roughly 20 doctoral programs (including 8 in English). [1] [3]  About 1,500 students who enter the university every year are international students. [4] \n",
      " The university operates multiple cultural and scientific museums, including the  University History Museum , a botanical garden, research centers, university libraries and a university press. The university is also affiliated with  Policlinico San Matteo , at which hundreds of medical students from the university perform  clinical rotations  during their clinical years.\n",
      " The University of Pavia is a member of the  COIMBRA Group  and  European University Association . It also participates in the  Erasmus Programme , which allows student exchanges between the University of Pavia and various universities in Europe. [5] \n",
      " \n",
      " \n",
      " History [ edit ] \n",
      " Foundation and the Middle Ages [ edit ] \n",
      " An edict issued by the  Frankish   king of Italy ,  Lothar I  (ruled 818–55) mentions the existence of a higher education institution at  Pavia  as early as AD 825. [6] [7]  This institution, mainly devoted to ecclesiastical and civil law as well as to divinity studies, was then selected as the prime educational centre for  northern Italy .\n"
     ]
    }
   ],
   "source": [
    "text0 = splits[0].page_content\n",
    "text1 = splits[1].page_content\n",
    "print(\"The first two chunks:\")\n",
    "print(\"-\" * 80)\n",
    "print(text0)\n",
    "print(\"-\" * 80)\n",
    "print(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate embeddings\n",
    "\n",
    "The langchain abstraction streamlines the process by combining the two key steps oc calculating the embeddings of documents and then loading these embeddings into a vector store. For the sake of the demonstration, we will calculate also manually the embedding of the first chunk. However, it's important to note that in typical usage, the VectorStore component of langchain simplifies this process.\n",
    "\n",
    "Please note: to execute the rest of the notebook, you will need an OpenAI API key. [Sign up](https://platform.openai.com/signup) at their website, you might get some [free credits](https://help.openai.com/en/articles/4936830-what-happens-after-i-use-my-free-tokens-or-the-3-months-is-up-in-the-free-trial). You will find your secret API key in the user settings page of their site, as [described here](https://help.openai.com/en/articles/4936850-where-do-i-find-my-api-key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ensure_openai_api_key\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "ensure_openai_api_key()\n",
    "embeddings_engine = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dimensions of the embedding: 1536\n",
      "The first 50 dimensions of the text embedding:\n",
      "[0.01615577055838958, -0.009743828633987588, 0.006967256996881327, -0.03690902632734008, -0.02615142696185595, 0.02213508519355134, -0.025105371000968837, -0.004729856886236512, -0.017033942581387643, -0.03794216856900001, -0.003615999941285602, -0.004520000101229991, 0.020934057151357155, -0.011919885318674151, 0.004239114135183526, -0.027559085687556373, -0.0023423285031480756, -0.00970508561366082, -0.010919028306404796, -0.028437255847909227, -0.0006679106947354245, 0.021308571772752444, -0.017214742101149087, -0.01664651424076517, -0.029754512951083716, 0.0037289998739671567, 0.003205971427862298, -0.0007324821430468326, 0.023762284596694768, -0.04070582741820169, -0.0038387713768419144, -0.018725714305957417, -0.028566398628116718, -0.00952428516257677, 0.021825142893582405, -0.007419256727607545, -0.012410628069727137, -0.011390399547294398, 0.02724914152494223, 0.02217382821387811, 0.02062411298874301, 0.007135142797415586, 0.0010500927787141896, -0.01314674266196792, -0.008794628361272186, 0.00015900713835235035, 0.018441599444442854, 0.007922914129210321, -0.006250513914803931, 0.018674057566403463]\n"
     ]
    }
   ],
   "source": [
    "emb0 = embeddings_engine.embed_documents([text0])[0]\n",
    "print(\"Number of dimensions of the embedding:\", len(emb0))\n",
    "print(\"The first 50 dimensions of the text embedding:\")\n",
    "print(emb0[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the embeddings into a vector store\n",
    "\n",
    "There are many vector databases available, both commercial and open source ones. For this demo, we will use [Chroma](https://www.trychroma.com), an open-source vector database that runs locally and is perfect for proof-of-concept and hobby projects.\n",
    "\n",
    "langchain already has an integration for Chroma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "!mkdir -p ./data\n",
    "# vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings_engine, persist_directory=\"./data\")\n",
    "\n",
    "# later on, you can load the vectorstore from disk with:\n",
    "vectorstore = Chroma(embedding_function=embeddings_engine, persist_directory=\"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the vector store\n",
    "\n",
    "We will ask the vector store to retrieve the semantically most relevant document chunk to a particular question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample question: What did Leonardo da Vinci studied at the University of Pavia?\n",
      "Most relevant chunk:\n",
      "--------------------------------------------------------------------------------\n",
      "Renaissance and Modern Period [ edit ] \n",
      " Towards the 15th century, prominent teachers such as  Baldo degli Ubaldi ,  Lorenzo Valla ,  Giasone del Maino  taught students in the fields of law, philosophy and literary studies. In the same years, Elia di Sabato da Fermo, personal doctor of  Filippo Maria Visconti , was the first professor of medicine of the Jewish religion at a European university, while from 1490 a teaching of Hebrew was established at the university. [12] \n",
      "Not many years later, probably in 1511,  Leonardo da Vinci  studied anatomy together with  Marcantonio della Torre , professor of anatomy at the university. [13] \n",
      "During the ongoing  Italian War of 1521-6 , the authorities in Pavia were forced to close the university in 1524. [6] [14]  However, during the 16th century, after the university was re-opened, scholars and scientists such as  Andrea Alciato  and  Gerolamo Cardano  taught here. During the period in which the  duchy of Milan  was governed by the kings of Spain, the research and educational activities of the university stagnated, but there were still prominent scholars such as  Gerolamo Saccheri  who was still involved with the university. [6] \n",
      " The rebirth of the university was, in part, due to the initiatives led by  Maria Theresa  and  Joseph II  of the  House of Austria , in the second half of the 18th century. The initiatives included massive renovations to the teaching programs, research and structure rehabilitations, which were still retained by the university until now. [6] \n",
      " \n",
      " Old Campus of the University of Pavia\n"
     ]
    }
   ],
   "source": [
    "sample_question = \"What did Leonardo da Vinci studied at the University of Pavia?\"\n",
    "results = vectorstore.similarity_search(sample_question, k=1)\n",
    "print(\"Sample question:\", sample_question)\n",
    "print(\"Most relevant chunk:\")\n",
    "print(\"-\" * 80)\n",
    "print(results[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the RAG application\n",
    "\n",
    "We will use [langchain framework](https://python.langchain.com/docs/get_started/introduction) to assemble our RAG application. Langchain provides an architectural framework for creating applications leverage Large Language Models (LLMs). It provides reasonable abstraction over different components (document loaders, embedding models, vector stores, language models, and many more) and makes it easy to create data pipelines (called \"chains\") from them. It also comes with the integration of many dozens of different commercial and open source implementations of the pipeline components, allowing you to create highly modular LLM applications.\n",
    "\n",
    "### Structuring the Prompt for LLMs\n",
    "\n",
    "When crafting a prompt for a Large Language Model (LLM), it's essential to consider various semantic elements. A prompt, essentially the \"question\" posed to the LLM, can include:\n",
    "\n",
    "- **Task Description**: Outlines what the task is.\n",
    "- **Tone of Voice**: Specifies the desired style or persona the model should adopt.\n",
    "- **Execution Instructions**: Provides detailed guidelines on how the task should be approached.\n",
    "- **Contextual Information**: Offers additional background relevant to the task.\n",
    "- **Example Solutions**: Presents examples to guide the expected response format.\n",
    "\n",
    "Prompt engineering is a specialized area in information technology focused on optimizing how prompts are structured for different LLMs. It involves developing best practices and strategies to create effective prompts.\n",
    "\n",
    "### Prompt Components in Retrieval Augmented Generation (RAG)\n",
    "\n",
    "For a RAG application, the prompt should ideally include:\n",
    "\n",
    "- **Model Persona/Tone of Voice**: Defines the identity or style the model should emulate.\n",
    "- **Task Description**: Clearly states the task, such as answering a question based on provided document chunks.\n",
    "- **Detailed Instructions**: Aims to minimize model-generated inaccuracies (hallucinations) and maintain answer conciseness.\n",
    "- **User Question**: The actual query or problem posed to the model.\n",
    "- **Extracted Document Chunks**: The relevant pieces of text the model uses to formulate its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt length engineering\n",
    "\n",
    "All current large language model have an upper bound of context length, i.e. the maximum length of the prompt you can send to them. In this demo, we will use OpenAI's [GPT-3.5](https://platform.openai.com/docs/models/gpt-3-5) model. This model has reasonable latency, pricing and cognitive performance for even complex Q&A tasks. It has a context size of 4096 tokens.\n",
    "\n",
    "It is also important to note that most of hosted LLM providers charge you for the number of tokens in the prompt (plus the number of tokens in the response) so you are stimulated to keep your prompts short. \n",
    "\n",
    "When we created the document chunks, we fixed the maximum length of a single chunk in 500 tokens. We plan to retrieve the 5 most relevant documents from the vector store, and add it to the prompt. Bellow you find a calculation that ensures the resulting prompt will fit in the context of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated upper bound of prompt length: 3060\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "encoder = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "raw_instructions = prompt.format(question=\"\", context=\"\")\n",
    "\n",
    "max_context_length = 4096\n",
    "number_of_documents_to_retrieve = 5\n",
    "maximum_question_length = 500  # This is an estimation and should be enforced from user interface\n",
    "raw_instructions_length = len(encoder.encode(raw_instructions))\n",
    "maximum_prompt_length = (\n",
    "    raw_instructions_length\n",
    "    + maximum_question_length\n",
    "    + number_of_documents_to_retrieve * chunk_size\n",
    ")\n",
    "print(\"Estimated upper bound of prompt length:\", maximum_prompt_length)\n",
    "assert maximum_prompt_length < max_context_length, \"The prompt is too long.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the RAG application\n",
    "\n",
    "We will use langchain wrapper around OpenAI client library to connect to GPT-3.5 hosted on OpenAI servers. Have your OpenAI API key ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following small helper function joins the retrieved document chunks into a single text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain defines a domain-specific language to describe LLM applications in a declarative way. This language, called \"Langchain Expression Language\" (LCEL) is a strict subset of Python and uses the pipe operator `|` to define data pipelines / chains in a similar concept as different linux shell do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What did Leonardo da Vinci studied at the University of Pavia?\n",
      "Answer: Leonardo da Vinci studied anatomy at the University of Pavia.\n"
     ]
    }
   ],
   "source": [
    "print(\"Question:\", sample_question)\n",
    "answer = rag_chain.invoke(sample_question)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dive deeper: https://python.langchain.com/docs/use_cases/question_answering/quickstart"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-rest-of-us",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
